{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset import FacialLandmarkDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_augmentations = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    ToTensorV2(),\n",
    "], keypoint_params=A.KeypointParams(format='xy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FacialLandmarkDataset(root_dir='archive/ibug_300W_large_face_landmark_dataset/afw', transform=basic_augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset (80% training, 20% testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for both sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, landmarks = dataset[10]\n",
    "\n",
    "if isinstance(img, torch.Tensor):\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "\n",
    "print(len(landmarks))\n",
    "\n",
    "# Convert to NumPy array for plotting\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Plot image\n",
    "plt.imshow(img_np)\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], c='r', marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(tensor, name=\"tensor\"):\n",
    "    if not torch.isfinite(tensor).all():\n",
    "        print(f\"{name} contains NaN or Inf values\")\n",
    "        print(\"Min:\", tensor.min().item(), \"Max:\", tensor.max().item())\n",
    "    #else:\n",
    "        #print(f\"{name} is valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(dataset.__len__()):\n",
    "    img, landmarks = dataset[idx]\n",
    "    landmarks_np = landmarks.numpy()\n",
    "    \n",
    "    if landmarks_np.shape[0] != 68:\n",
    "        print(len(landmarks))\n",
    "        print(f\"Face at index {idx} does not have exactly 68 landmarks. Found: {landmarks_np.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiducial_focus_augmentation(image_tensor, landmarks, n, epoch, total_epochs):\n",
    "    augmented_images = []\n",
    "    patch_size = max(1, n - (epoch * n // total_epochs))  # Gradually reduce patch size\n",
    "\n",
    "    for i in range(image_tensor.size(0)):  # Iterate over batch\n",
    "        image = image_tensor[i].permute(1, 2, 0).cpu().numpy()  # Convert tensor to NumPy\n",
    "        augmented_image = image.copy()\n",
    "        image_landmarks = landmarks[i]\n",
    "\n",
    "        for (x, y) in image_landmarks:\n",
    "            x, y = int(x), int(y)\n",
    "            top_left = (max(0, x - patch_size // 2), max(0, y - patch_size // 2))\n",
    "            bottom_right = (min(image.shape[1], x + patch_size // 2), min(image.shape[0], y + patch_size // 2))\n",
    "\n",
    "            # Apply black patch\n",
    "            augmented_image[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]] = (0, 0, 0)\n",
    "\n",
    "        augmented_images.append(torch.from_numpy(augmented_image).permute(2, 0, 1))\n",
    "\n",
    "    return torch.stack(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BlurPool Layer for anti-aliasing\n",
    "class BlurPool(nn.Module):\n",
    "    def __init__(self, channels, stride=1):\n",
    "        super(BlurPool, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "        self.blur_kernel = torch.tensor([[1., 2., 1.],\n",
    "                                         [2., 4., 2.],\n",
    "                                         [1., 2., 1.]])\n",
    "        self.blur_kernel = self.blur_kernel / self.blur_kernel.sum()\n",
    "        self.blur_kernel = self.blur_kernel.expand(channels, 1, 3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        return F.conv2d(x, self.blur_kernel.to(x.device), stride=self.stride, groups=x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement FF-Parser Layer to filter high-frequency noise\n",
    "class FFParser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFParser, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply FFT to transform to frequency domain\n",
    "        x_freq = torch.fft.fft2(x, dim=(-2, -1))\n",
    "        # Apply a low-pass filter (keeping low-frequency components)\n",
    "        x_filtered = torch.fft.ifft2(x_freq, dim=(-2, -1)).real\n",
    "        return x_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourglass module with anti-aliasing and noise reduction\n",
    "class HourglassModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HourglassModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.anti_alias = BlurPool(128, stride=2)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.ff_parser = FFParser()  # Add FF-Parser layer for noise reduction\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.anti_alias(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ff_parser(x)  # Apply FF-Parser to filter out noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facial Landmark Model\n",
    "class FacialLandmarkModel(nn.Module):\n",
    "    def __init__(self, num_landmarks):\n",
    "        super(FacialLandmarkModel, self).__init__()\n",
    "        self.vit_backbone = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.vit_backbone.head = nn.Identity()  # Remove classification head\n",
    "        self.reshape = nn.Linear(768, 256 * 14 * 14)\n",
    "        self.hourglass = HourglassModule()\n",
    "        self.fc = nn.Conv2d(128, num_landmarks, kernel_size=1)  # Predict heatmaps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x, min=0.0, max=1.0)  # Clamp input values\n",
    "        features = self.vit_backbone(x)\n",
    "        features = self.reshape(features).view(-1, 256, 14, 14)\n",
    "        features = self.hourglass(features)\n",
    "        heatmaps = self.fc(features)\n",
    "        return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_soft_argmax(heatmap, temperature=1.0):\n",
    "    softmax = F.softmax(heatmap.view(-1) / temperature, dim=0)  # Flatten heatmap and apply softmax\n",
    "    height, width = heatmap.shape\n",
    "\n",
    "    # Create a meshgrid of coordinates corresponding to the heatmap size\n",
    "    y_grid, x_grid = torch.meshgrid(torch.arange(height), torch.arange(width), indexing='ij')\n",
    "\n",
    "    # Normalize the softmax-weighted sum to get coordinates in the heatmap space\n",
    "    x_coord = (x_grid.float() * softmax.view(height, width)).sum()\n",
    "    y_coord = (y_grid.float() * softmax.view(height, width)).sum()\n",
    "\n",
    "    return x_coord, y_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmark_coords_from_heatmaps(heatmaps, image_size=(224, 224)):\n",
    "    batch_size, num_landmarks, height, width = heatmaps.size()\n",
    "    coords = torch.zeros(batch_size, num_landmarks, 2, dtype=torch.float32)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for l in range(num_landmarks):\n",
    "            heatmap = heatmaps[b, l]\n",
    "            if heatmap.sum() > 0:\n",
    "                heatmap = heatmap / heatmap.sum()  # Normalize the heatmap\n",
    "            else:\n",
    "                heatmap = heatmap\n",
    "\n",
    "            # Extract the coordinates from the heatmap using local_soft_argmax\n",
    "            x, y = local_soft_argmax(heatmap)\n",
    "\n",
    "            # Scale coordinates back to the original image resolution\n",
    "            coords[b, l, 0] = torch.clamp(x * image_size[1] / width, min=0, max=image_size[1] - 1)\n",
    "            coords[b, l, 1] = torch.clamp(y * image_size[0] / height, min=0, max=image_size[0] - 1)\n",
    "\n",
    "    \n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCCA Loss Function for consistency\n",
    "class DCCALoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCCALoss, self).__init__()\n",
    "\n",
    "    def forward(self, H1, H2):\n",
    "        H1 = H1 - H1.mean(dim=0, keepdim=True)\n",
    "        H2 = H2 - H2.mean(dim=0, keepdim=True)\n",
    "        H1H2_cov = (H1.T @ H2) / (H1.size(0) - 1)\n",
    "        H1_var = (H1.T @ H1) / (H1.size(0) - 1)\n",
    "        H2_var = (H2.T @ H2) / (H2.size(0) - 1)\n",
    "        H1_var_sqrt = torch.sqrt(torch.trace(H1_var))\n",
    "        H2_var_sqrt = torch.sqrt(torch.trace(H2_var))\n",
    "        corr = torch.trace(H1H2_cov) / (H1_var_sqrt * H2_var_sqrt)\n",
    "        return -corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FacialLandmarkModel(num_landmarks=68).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "dcca_loss = DCCALoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250  # Increase the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, landmarks = batch\n",
    "        images, landmarks = images.to(device), landmarks.to(device)\n",
    "\n",
    "        # Apply fiducial focus augmentation\n",
    "        fiducial_focus_images = fiducial_focus_augmentation(images, landmarks, 8, epoch, num_epochs)\n",
    "\n",
    "        # Forward pass for original and transformed images\n",
    "        outputs_original = model(images)\n",
    "        outputs_transformed = model(fiducial_focus_images)\n",
    "\n",
    "        # Extract coordinates from heatmaps\n",
    "        coords_original = extract_landmark_coords_from_heatmaps(outputs_original)\n",
    "        coords_transformed = extract_landmark_coords_from_heatmaps(outputs_transformed)\n",
    "\n",
    "        # Flatten the coordinates for DCCA loss\n",
    "        coords_original = coords_original.view(coords_original.size(0), -1)\n",
    "        coords_transformed = coords_transformed.view(coords_transformed.size(0), -1)\n",
    "\n",
    "        # DCCA Loss (consistency loss between original and transformed outputs)\n",
    "        loss_dcca = dcca_loss(coords_original, coords_transformed)\n",
    "\n",
    "        # Calculate the MSE loss with ground truth landmarks\n",
    "        ground_truth_loss = F.mse_loss(coords_original.view(-1, 68, 2), landmarks)\n",
    "\n",
    "        # Total loss: weighted sum of DCCA and ground truth MSE losses\n",
    "        loss = 0.5 * loss_dcca + 0.5 * ground_truth_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'best_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NME calculation\n",
    "def calculate_nme(preds, labels, normalization_factor):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Mean Error (NME)\n",
    "    preds: Tensor of shape (batch_size, num_landmarks, 2) - predicted landmarks\n",
    "    labels: Tensor of shape (batch_size, num_landmarks, 2) - ground truth landmarks\n",
    "    normalization_factor: Tensor or value representing the normalization factor (e.g., inter-ocular distance)\n",
    "    \"\"\"\n",
    "    batch_size, num_landmarks, _ = preds.shape\n",
    "    error = torch.norm(preds - labels, dim=-1)  # Euclidean distance for each landmark\n",
    "    nme = error.mean(dim=1) / normalization_factor  # Mean error per image, normalized\n",
    "    return nme.mean()  # Return the average NME across the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function with MSE and NME\n",
    "def evaluate_model(model, test_loader, normalization_factors):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_nme = 0.0\n",
    "    criterion = nn.MSELoss()  # Using MSE loss for evaluation\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for batch_idx, (images, landmarks) in enumerate(test_loader):\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "            outputs = model(images)\n",
    "            coords_pred = extract_landmark_coords_from_heatmaps(outputs)\n",
    "\n",
    "            # Calculate MSE Loss\n",
    "            loss = criterion(coords_pred, landmarks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate NME\n",
    "            normalization_factor = normalization_factors[batch_idx]\n",
    "            nme_batch = calculate_nme(coords_pred, landmarks, normalization_factor)\n",
    "            total_nme += nme_batch.item()\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_loader)\n",
    "    avg_nme = total_nme / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss (MSE): {avg_test_loss:.4f}\")\n",
    "    print(f\"Test NME: {avg_nme:.4f}\")\n",
    "    return avg_test_loss, avg_nme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions function with better visualization\n",
    "def plot_predictions(model, test_loader, image_size=(224, 224), num_images=5):\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            coords_pred = extract_landmark_coords_from_heatmaps(outputs, image_size).cpu().numpy()\n",
    "            images_np = images.permute(0, 2, 3, 1).cpu().numpy()  # Convert to HWC format for plotting\n",
    "\n",
    "            # Plot a few images with their predicted landmarks\n",
    "            for i in range(len(images)):\n",
    "                plt.imshow(images_np[i])\n",
    "                plt.scatter(coords_pred[i][:, 0], coords_pred[i][:, 1], c='r', marker='x', label='Predicted')\n",
    "                plt.scatter(landmarks[i][:, 0], landmarks[i][:, 1], c='g', marker='o', label='Ground Truth')\n",
    "                plt.title(f\"Predicted vs Ground Truth Landmarks\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                images_shown += 1\n",
    "                if images_shown >= num_images:\n",
    "                    return  # Stop after showing the specified number of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function to get normalization factors (e.g., inter-ocular distance)\n",
    "def get_normalization_factors(landmarks_batch):\n",
    "    \"\"\"\n",
    "    Compute the inter-ocular distance or any other normalization factor\n",
    "    landmarks_batch: Tensor of shape (batch_size, num_landmarks, 2)\n",
    "    Return: Tensor of shape (batch_size,) representing normalization factor for each image\n",
    "    \"\"\"\n",
    "    left_eye = landmarks_batch[:, [36], :]  # Coordinates for left eye\n",
    "    right_eye = landmarks_batch[:, [45], :]  # Coordinates for right eye\n",
    "    normalization_factors = torch.norm(left_eye - right_eye, dim=-1)  # Inter-ocular distance\n",
    "    return normalization_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calling evaluation functions, ensure normalization factors are available for each batch\n",
    "normalization_factors = []\n",
    "\n",
    "for images, landmarks in test_loader:\n",
    "    norm_factors = get_normalization_factors(landmarks)\n",
    "    normalization_factors.append(norm_factors)\n",
    "\n",
    "# Evaluate and plot predictions after training\n",
    "avg_test_loss, avg_nme = evaluate_model(model, test_loader, normalization_factors)\n",
    "plot_predictions(model, test_loader, num_images=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
