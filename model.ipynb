{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import itertools\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from dataset import FacialLandmarkDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_augmentations = A.Compose([\n",
    "    A.Resize(224, 224),  # Resize images to 512x512\n",
    "    A.HorizontalFlip(p=0.5),  # 50% probability of horizontal flipping\n",
    "    A.RandomBrightnessContrast(p=0.2),  # Simulate grayscale by altering brightness\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=30, p=0.5),  # Translation, scaling, rotation\n",
    "    ToTensorV2(),  # Convert to PyTorch Tensor\n",
    "], keypoint_params=A.KeypointParams(format='xy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FacialLandmarkDataset(root_dir='archive/ibug_300W_large_face_landmark_dataset/afw', transform=basic_augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset (80% training, 20% testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for both sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, landmarks = dataset[59]\n",
    "\n",
    "if isinstance(img, torch.Tensor):\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "\n",
    "print(len(landmarks))\n",
    "\n",
    "# Convert to NumPy array for plotting\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Plot image\n",
    "plt.imshow(img_np)\n",
    "plt.scatter(landmarks[:, 0], landmarks[:, 1], c='r', marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(dataset.__len__()):\n",
    "    img, landmarks = dataset[idx]\n",
    "    landmarks_np = landmarks.numpy()\n",
    "    \n",
    "    if landmarks_np.shape[0] != 68:\n",
    "        print(len(landmarks))\n",
    "        print(f\"Face at index {idx} does not have exactly 68 landmarks. Found: {landmarks_np.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookahead wrapper for PyTorch optimizers\n",
    "class Lookahead(torch.optim.Optimizer):\n",
    "    def __init__(self, base_optimizer, k=5, alpha=0.5):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid alpha parameter: {alpha}')\n",
    "        if not k >= 1:\n",
    "            raise ValueError(f'Invalid k parameter: {k}')\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.slow_weights = [[p.clone().detach() for p in group['params']]\n",
    "                             for group in self.param_groups]\n",
    "        for w in itertools.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        self.counter += 1\n",
    "        if self.counter % self.k == 0:\n",
    "            for group, slow_weights in zip(self.param_groups, self.slow_weights):\n",
    "                for p, q in zip(group['params'], slow_weights):\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    q.data.add_(self.alpha, p.data - q.data)\n",
    "                    p.data.copy_(q.data)\n",
    "        return loss\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.base_optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCCALoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCCALoss, self).__init__()\n",
    "\n",
    "    def forward(self, H1, H2):\n",
    "        H1 = H1 - H1.mean(dim=0, keepdim=True)\n",
    "        H2 = H2 - H2.mean(dim=0, keepdim=True)\n",
    "        H1H2_cov = (H1.T @ H2) / (H1.size(0) - 1)\n",
    "        H1_var = (H1.T @ H1) / (H1.size(0) - 1)\n",
    "        H2_var = (H2.T @ H2) / (H2.size(0) - 1)\n",
    "        H1_var_sqrt = torch.sqrt(torch.trace(H1_var))\n",
    "        H2_var_sqrt = torch.sqrt(torch.trace(H2_var))\n",
    "        corr = torch.trace(H1H2_cov) / (H1_var_sqrt * H2_var_sqrt)\n",
    "        return -corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_soft_argmax(heatmap, temperature=0.5):\n",
    "    softmax = F.softmax(heatmap.view(-1) / temperature, dim=0)\n",
    "    height, width = heatmap.shape\n",
    "    softmax = softmax.view(height, width)\n",
    "    \n",
    "    y_grid, x_grid = torch.meshgrid(torch.arange(height), torch.arange(width), indexing='ij')\n",
    "    \n",
    "    x_coord = (x_grid.float() * softmax).sum()\n",
    "    y_coord = (y_grid.float() * softmax).sum()\n",
    "    \n",
    "    return x_coord, y_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmark_coords_from_heatmaps(heatmaps, image_size=(224, 224)):\n",
    "    batch_size, num_landmarks, height, width = heatmaps.size()\n",
    "    coords = torch.zeros(batch_size, num_landmarks, 2, dtype=torch.float32).to(heatmaps.device)\n",
    "    for b in range(batch_size):\n",
    "        for l in range(num_landmarks):\n",
    "            heatmap = heatmaps[b, l]\n",
    "            if heatmap.sum() > 0:\n",
    "                heatmap = heatmap / heatmap.sum()\n",
    "            x, y = local_soft_argmax(heatmap)\n",
    "            coords[b, l, 0] = torch.clamp(x * image_size[1] / width, min=0, max=image_size[1] - 1)\n",
    "            coords[b, l, 1] = torch.clamp(y * image_size[0] / height, min=0, max=image_size[0] - 1)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiducial Focus Augmentation\n",
    "def fiducial_focus_augmentation(image_tensor, landmarks, n, epoch, total_epochs):\n",
    "    augmented_images = []\n",
    "    patch_size = max(1, n - (epoch * n // total_epochs))  # Gradually reduce patch size\n",
    "\n",
    "    for i in range(image_tensor.size(0)):  # Iterate over batch\n",
    "        image = image_tensor[i].permute(1, 2, 0).cpu().numpy()  # Convert tensor to NumPy\n",
    "        augmented_image = image.copy()\n",
    "        image_landmarks = landmarks[i]\n",
    "\n",
    "        for (x, y) in image_landmarks:\n",
    "            x, y = int(x), int(y)\n",
    "            top_left = (max(0, x - patch_size // 2), max(0, y - patch_size // 2))\n",
    "            bottom_right = (min(image.shape[1], x + patch_size // 2), min(image.shape[0], y + patch_size // 2))\n",
    "\n",
    "            # Apply black patch\n",
    "            augmented_image[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]] = (0, 0, 0)\n",
    "\n",
    "        augmented_images.append(torch.from_numpy(augmented_image).permute(2, 0, 1))\n",
    "\n",
    "    return torch.stack(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurPool(nn.Module):\n",
    "    def __init__(self, channels, stride=1):\n",
    "        super(BlurPool, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "        self.blur_kernel = torch.tensor([[1., 2., 1.], [2., 4., 2.], [1., 2., 1.]])\n",
    "        self.blur_kernel = self.blur_kernel / self.blur_kernel.sum()\n",
    "        self.blur_kernel = self.blur_kernel.expand(channels, 1, 3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        return F.conv2d(x, self.blur_kernel.to(x.device), stride=self.stride, groups=x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFParser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFParser, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_freq = torch.fft.fft2(x, dim=(-2, -1))\n",
    "        x_filtered = torch.fft.ifft2(x_freq, dim=(-2, -1)).real\n",
    "        return x_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourglassModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HourglassModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.anti_alias = BlurPool(128, stride=1)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.ff_parser = FFParser()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.anti_alias(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ff_parser(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # Kaiming Uniform for Conv layers (good for ReLU activations)\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        elif isinstance(m, nn.Linear):\n",
    "            # Xavier Uniform for Linear layers (good for non-ReLU activations)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            # BatchNorm layers are typically initialized with weight=1 and bias=0\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            # LayerNorm initialization (set weights to 1 and biases to 0)\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            # Embeddings should be initialized with uniform distribution\n",
    "            nn.init.uniform_(m.weight, -0.05, 0.05)\n",
    "        \n",
    "        elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
    "            # Initialize LSTM/GRU layers: Orthogonal initialization for recurrent weights\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialLandmarkModel(nn.Module):\n",
    "    def __init__(self, num_landmarks):\n",
    "        super(FacialLandmarkModel, self).__init__()\n",
    "        \n",
    "        # ViT Backbone with pretrained weights\n",
    "        self.vit_backbone = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.vit_backbone.head = nn.Identity()  # Removing the classification head\n",
    "\n",
    "        # Reshape the output of the ViT backbone to match the input size for the hourglass module\n",
    "        self.reshape = nn.Linear(768, 256 * 14 * 14)\n",
    "        \n",
    "        # Adding dropout after the reshaping\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        # Hourglass module for feature extraction and localization\n",
    "        self.hourglass = HourglassModule()\n",
    "\n",
    "        # Final output convolution to predict heatmaps for the landmarks\n",
    "        self.fc = nn.Conv2d(128, num_landmarks, kernel_size=1)\n",
    "        \n",
    "        # Initialize weights of the model\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Clamping input values to [0, 1]\n",
    "        x = torch.clamp(x, min=0.0, max=1.0)\n",
    "        \n",
    "        # Extract features using the ViT backbone\n",
    "        features = self.vit_backbone(x)\n",
    "        \n",
    "        # Reshape the ViT output and apply dropout\n",
    "        features = self.reshape(features).view(-1, 256, 14, 14)\n",
    "        features = self.dropout(features)  # Applying dropout here\n",
    "\n",
    "        # Pass through the hourglass module\n",
    "        features = self.hourglass(features)\n",
    "\n",
    "        # Predict heatmaps for landmarks\n",
    "        heatmaps = self.fc(features)\n",
    "        \n",
    "        return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FacialLandmarkModel(num_landmarks=68).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Scheduler\n",
    "base_optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "lookahead_optimizer = Lookahead(base_optimizer, k=5, alpha=0.5)\n",
    "scheduler = CosineAnnealingWarmRestarts(base_optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# Gradient Scaler for mixed precision\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "dcca_loss = DCCALoss().to(device)\n",
    "criterion_mse = nn.MSELoss().to(device)\n",
    "\n",
    "max_grad_norm = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation step\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks in val_loader:\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted_coords = extract_landmark_coords_from_heatmaps(outputs)\n",
    "            mse_loss = criterion_mse(predicted_coords.view(-1, 68, 2), landmarks)\n",
    "            val_loss += mse_loss.item()\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para plotar as previsões\n",
    "def plot_predictions_during_training(images, coords_pred, landmarks, fiducial_focus_images, epoch, num_images=3):\n",
    "    images_np = images.permute(0, 2, 3, 1).cpu().numpy()  # Converter para HWC para plotagem\n",
    "    coords_pred_np = coords_pred.cpu().numpy()\n",
    "\n",
    "    # Para as imagens com FiFA\n",
    "    fiducial_images_np = fiducial_focus_images.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(num_images, 2, figsize=(10, 5 * num_images))  # Organiza as imagens em linhas\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Imagem original com previsões\n",
    "        axs[i, 0].imshow(images_np[i])\n",
    "        axs[i, 0].scatter(coords_pred_np[i][:, 0], coords_pred_np[i][:, 1], c='r', marker='x', label='Predicted')\n",
    "        axs[i, 0].scatter(landmarks[i][:, 0].cpu(), landmarks[i][:, 1].cpu(), c='g', marker='o', label='Ground Truth')\n",
    "        axs[i, 0].set_title(f\"Original Image - Epoch {epoch}\")\n",
    "        axs[i, 0].legend()\n",
    "\n",
    "        # Imagem com FiFA\n",
    "        axs[i, 1].imshow(fiducial_images_np[i])\n",
    "        axs[i, 1].scatter(coords_pred_np[i][:, 0], coords_pred_np[i][:, 1], c='r', marker='x', label='Predicted')\n",
    "        axs[i, 1].scatter(landmarks[i][:, 0].cpu(), landmarks[i][:, 1].cpu(), c='g', marker='o', label='Ground Truth')\n",
    "        axs[i, 1].set_title(f\"Fiducial Focus Augmentation - Epoch {epoch}\")\n",
    "        axs[i, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_dcca_loss = 0.0\n",
    "    running_mse_loss = 0.0\n",
    "\n",
    "    for images, landmarks in train_loader:\n",
    "        images, landmarks = images.to(device), landmarks.to(device)\n",
    "\n",
    "        fiducial_focus_images = fiducial_focus_augmentation(images, landmarks, 20, epoch, num_epochs)\n",
    "\n",
    "        # Reset gradients\n",
    "        lookahead_optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision\n",
    "        with autocast():\n",
    "            # Forward pass for original and augmented images\n",
    "            outputs_original = model(images)\n",
    "            outputs_transformed = model(fiducial_focus_images)\n",
    "\n",
    "            # Extract coordinates from heatmaps\n",
    "            coords_original = extract_landmark_coords_from_heatmaps(outputs_original)\n",
    "            coords_transformed = extract_landmark_coords_from_heatmaps(outputs_transformed)\n",
    "\n",
    "            # Reshape for DCCA loss calculation\n",
    "            coords_original_flat = coords_original.view(coords_original.size(0), -1)\n",
    "            coords_transformed_flat = coords_transformed.view(coords_transformed.size(0), -1)\n",
    "\n",
    "            # Calculate DCCA loss and MSE loss\n",
    "            loss_dcca = dcca_loss(coords_original_flat, coords_transformed_flat)\n",
    "            loss_mse = criterion_mse(coords_original.view(-1, 68, 2), landmarks)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = 0.2 * loss_dcca + 0.8 * loss_mse\n",
    "\n",
    "        # Mixed precision scaling of the loss\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(lookahead_optimizer)  # Unscales gradients before clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        # Optimizer step with mixed precision scaling\n",
    "        scaler.step(lookahead_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_dcca_loss += loss_dcca.item()\n",
    "        running_mse_loss += loss_mse.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_dcca_loss = running_dcca_loss / len(train_loader)\n",
    "    avg_mse_loss = running_mse_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {avg_loss:.4f} (DCCA: {avg_dcca_loss:.4f}, MSE: {avg_mse_loss:.4f})\")\n",
    "\n",
    "    # Validação\n",
    "    val_loss = validate_model(model, test_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if (epoch + 50) % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            images, landmarks = next(iter(test_loader))\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "\n",
    "            # Generate the outputs and predictions\n",
    "            outputs = model(images)\n",
    "            coords_pred = extract_landmark_coords_from_heatmaps(outputs)\n",
    "\n",
    "            # Apply Fiducial Focus Augmentation to test images\n",
    "            fiducial_focus_images = fiducial_focus_augmentation(images, landmarks, n=20, epoch=epoch, total_epochs=num_epochs)\n",
    "\n",
    "            # Visualizar previsões e mapas de calor\n",
    "            plot_predictions_during_training(images, coords_pred, landmarks, fiducial_focus_images, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'best_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function to get normalization factors (e.g., inter-ocular distance)\n",
    "def get_normalization_factors(landmarks_batch):\n",
    "    left_eye = landmarks_batch[:, 36, :]  # Coordinates for left eye (landmark index 36)\n",
    "    right_eye = landmarks_batch[:, 45, :]  # Coordinates for right eye (landmark index 45)\n",
    "    normalization_factors = torch.norm(left_eye - right_eye, dim=-1)  # Inter-ocular distance\n",
    "    return normalization_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NME calculation\n",
    "def calculate_nme(preds, labels, normalization_factor):\n",
    "    error = torch.norm(preds - labels, dim=-1)  # Euclidean distance for each landmark\n",
    "    nme = error.mean(dim=1) / normalization_factor  # Mean error per image, normalized\n",
    "    return nme.mean()  # Return the average NME across the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_nme = 0.0\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks in test_loader:\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Extract predicted coordinates from heatmaps\n",
    "            predicted_coords = extract_landmark_coords_from_heatmaps(outputs)\n",
    "\n",
    "            # Calculate MSE loss with ground truth landmarks\n",
    "            mse_loss = criterion(predicted_coords.view(-1, 68, 2), landmarks)\n",
    "\n",
    "            # Compute normalization factors for the current batch\n",
    "            norm_factors = get_normalization_factors(landmarks)\n",
    "\n",
    "            # Calculate NME for the current batch\n",
    "            nme = calculate_nme(predicted_coords.view(-1, 68, 2), landmarks, norm_factors)\n",
    "\n",
    "            total_loss += mse_loss.item()\n",
    "            total_nme += nme.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_nme = total_nme / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss (MSE): {avg_loss:.4f}, NME: {avg_nme:.4f}\")\n",
    "    return avg_loss, avg_nme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions function with better visualization\n",
    "def plot_predictions(model, test_loader, image_size=(224, 224), num_images=5):\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            coords_pred = extract_landmark_coords_from_heatmaps(outputs, image_size).cpu().numpy()\n",
    "            images_np = images.permute(0, 2, 3, 1).cpu().numpy()  # Convert to HWC format for plotting\n",
    "\n",
    "            # Plot a few images with their predicted landmarks\n",
    "            for i in range(len(images)):\n",
    "                plt.imshow(images_np[i])\n",
    "                plt.scatter(coords_pred[i][:, 0], coords_pred[i][:, 1], c='r', marker='x', label='Predicted')\n",
    "                plt.scatter(landmarks[i][:, 0], landmarks[i][:, 1], c='g', marker='o', label='Ground Truth')\n",
    "                plt.title(f\"Predicted vs Ground Truth Landmarks\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                images_shown += 1\n",
    "                if images_shown >= num_images:\n",
    "                    return  # Stop after showing the specified number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call evaluation and plotting functions after training\n",
    "avg_test_loss, avg_nme = evaluate_model(model, test_loader)\n",
    "plot_predictions(model, test_loader, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
